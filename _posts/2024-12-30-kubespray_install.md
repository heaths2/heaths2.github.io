---
title: Kubespray ì„¤ì¹˜ë°©ë²•
author: G.G
date: 2024-12-30 11:56:00 +0900
categories: [Blog, Orchestration]
tags: [Kubernetes, K8s, Kubespray]
---

## ğŸ“˜ ê°œìš”
KubesprayëŠ” Ansible ê¸°ë°˜ì˜ ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ë¡œ, Kubernetes í´ëŸ¬ìŠ¤í„°ë¥¼ ë¹ ë¥´ê³  ì‰½ê²Œ ì„¤ì¹˜í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤. íŠ¹íˆ ê³ ê°€ìš©ì„±(High Availability, HA) í´ëŸ¬ìŠ¤í„°ë¥¼ êµ¬ì¶•í•˜ëŠ” ë° ì í•©í•˜ë©°, ë‹¤ì–‘í•œ í´ë¼ìš°ë“œ ë° ë² ì–´ë©”íƒˆ í™˜ê²½ì—ì„œ ë™ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## íŠ¹ì§•
1. ê³ ê°€ìš©ì„±(HA)
- Control Planeê³¼ etcd í´ëŸ¬ìŠ¤í„°ì˜ ê³ ê°€ìš©ì„±ì„ ë³´ì¥.
- ë‹¤ì¤‘ Control Plane ë…¸ë“œë¥¼ í†µí•´ ì¥ì•  ì‹œì—ë„ Kubernetes API ì„œë²„ê°€ ì§€ì†ì ìœ¼ë¡œ ë™ì‘.
- ë‹¤ì¤‘ etcd ë…¸ë“œ êµ¬ì„±ìœ¼ë¡œ ë°ì´í„° ì•ˆì •ì„±ê³¼ ë³µêµ¬ ëŠ¥ë ¥ì„ ë³´ì¥.

2. í™•ì¥ì„±
- ë…¸ë“œ ì¶”ê°€ ë° ì œê±°ë¥¼ ì‰½ê²Œ ìˆ˜í–‰ ê°€ëŠ¥.
- ë‹¤ì–‘í•œ ë„¤íŠ¸ì›Œí¬ í”ŒëŸ¬ê·¸ì¸(Calico, Flannel, Cilium ë“±) ì§€ì›.

3. ìœ ì—°ì„±
- ë‹¤ì–‘í•œ ìš´ì˜ì²´ì œ(Ubuntu, CentOS, RHEL ë“±) ì§€ì›.
- ì‚¬ìš©ì ì •ì˜ê°€ ìš©ì´í•˜ë©°, ì„¤ì • íŒŒì¼ì„ í†µí•´ ë„¤íŠ¸ì›Œí¬, ì¸ì¦ ë“± ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥.

4. ìë™í™”
- Ansible í”Œë ˆì´ë¶ì„ ê¸°ë°˜ìœ¼ë¡œ ì„¤ì¹˜ ë° ê´€ë¦¬ ìë™í™”.
- Kubernetesì™€ ê´€ë ¨ëœ ëª¨ë“  ì»´í¬ë„ŒíŠ¸(apiserver, scheduler, controller-manager, etcd ë“±)ë¥¼ ìë™ ì„¤ì •.

## êµ¬ì„±ìš”ì†Œ
1. Control Plane
- Kubernetes í´ëŸ¬ìŠ¤í„°ì˜ ì¤‘ì‹¬ ì»´í¬ë„ŒíŠ¸.
- ì£¼ìš” êµ¬ì„± ìš”ì†Œ:
  - kube-apiserver: Kubernetes API ìš”ì²­ì„ ì²˜ë¦¬.
  - kube-controller-manager: í´ëŸ¬ìŠ¤í„° ìƒíƒœë¥¼ ì¡°ì •.
  - kube-scheduler: ì›Œí¬ë¡œë“œë¥¼ ì ì ˆí•œ ë…¸ë“œì— ìŠ¤ì¼€ì¤„ë§.
  - etcd: í´ëŸ¬ìŠ¤í„° ìƒíƒœ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ë¶„ì‚° í‚¤-ê°’ ì €ì¥ì†Œ.
- HAë¥¼ ìœ„í•œ êµ¬ì„±
  - Load Balancer(Haproxy/Keepalived ë“±)ì™€ VIPë¥¼ í†µí•´ Control Plane ë…¸ë“œ ê°„ ë¡œë“œ ë¶„ì‚°.
2. etcd
- Kubernetes í´ëŸ¬ìŠ¤í„° ìƒíƒœ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ë¶„ì‚° í‚¤-ê°’ ì €ì¥ì†Œ.
- HAë¥¼ ìœ„í•´ ë‹¤ì¤‘ ë…¸ë“œë¡œ êµ¬ì„±
  - --etcd-serversì— ë‹¤ìˆ˜ì˜ etcd ë…¸ë“œë¥¼ ì§€ì •í•˜ì—¬ ì¥ì•  ì‹œì—ë„ ë°ì´í„° ë³µêµ¬ ê°€ëŠ¥.
3. Load Balancer
- Control Plane ë…¸ë“œë¡œ íŠ¸ë˜í”½ì„ ë¶„ì‚°.
- VIP(Virtual IP)ë¥¼ í™œìš©í•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ê°€ ë‹¨ì¼ ì—”ë“œí¬ì¸íŠ¸ë¡œ ì ‘ì† ê°€ëŠ¥.

4. Kubespray êµ¬ì„± íŒŒì¼
- inventory.ini: ë…¸ë“œ ì—­í•  ì •ì˜(Control Plane, etcd, Worker ë“±).
- group_vars/all.yml: ê¸€ë¡œë²Œ ë³€ìˆ˜ ì •ì˜(Kubernetes ë²„ì „, ë„¤íŠ¸ì›Œí¬ ì„¤ì • ë“±).
- group_vars/k8s_cluster/addons.yml: ì¶”ê°€ ì»´í¬ë„ŒíŠ¸(dashboard, metrics-server ë“±) ì„¤ì •.

## Kubernetes ë¦¬ì†ŒìŠ¤ ìš”ì•½
ë¦¬ì†ŒìŠ¤ë³„ ëª©ì , ê´€ë ¨ ë¦¬ì†ŒìŠ¤, ëª…ë ¹ì–´, ê°œë…

| **ë¦¬ì†ŒìŠ¤**                      | **ëª©ì **                                                                                         | **ê´€ë ¨ëœ ë¦¬ì†ŒìŠ¤**                     | **ëª…ë ¹ì–´**                  | **ê°œë… ìš”ì•½**                                                                                                                                                                 |
|---------------------------------|-------------------------------------------------------------------------------------------------|---------------------------------------|-----------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| PVC (Persistent Volume Claim)   | ìŠ¤í† ë¦¬ì§€ë¥¼ ìš”ì²­í•˜ëŠ” ë¦¬ì†ŒìŠ¤                                                                 | PV                                   | `kubectl get pvc`           | Podê°€ í•„ìš”í•œ ìŠ¤í† ë¦¬ì§€ë¥¼ ìš”ì²­í•˜ëŠ” ì„ ì–¸. "ì´ë§Œí¼ì˜ ìŠ¤í† ë¦¬ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤!" ë¼ëŠ” ìš”ì²­.                                                                                        |
| PV (Persistent Volume)          | PVC ìš”ì²­ì— ë”°ë¼ ìŠ¤í† ë¦¬ì§€ë¥¼ ì œê³µí•˜ëŠ” ë¦¬ì†ŒìŠ¤                                                       | ì‹¤ì œ ìŠ¤í† ë¦¬ì§€ (EBS, NFS, ë¡œì»¬ ë””ìŠ¤í¬) | `kubectl get pv`            | PVCê°€ ìš”ì²­í•œ ìŠ¤í† ë¦¬ì§€ë¥¼ ì—°ê²°í•˜ëŠ” ì‹¤ì œ ì €ì¥ì†Œ. Kubernetesê°€ ë¬¼ë¦¬ì  ìŠ¤í† ë¦¬ì§€ë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì¶”ìƒí™” ë ˆì´ì–´.                                                                  |
| SVC (Service)                   | Pod ê°„ ë˜ëŠ” Podì™€ ì™¸ë¶€ ë„¤íŠ¸ì›Œí¬ ê°„ì˜ í†µì‹ ì„ ê´€ë¦¬                                                   | Pod, Endpoint                       | `kubectl get svc`           | Podì˜ ë„¤íŠ¸ì›Œí¬ ì ‘ì ì„ ì œê³µ. í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ ë˜ëŠ” ì™¸ë¶€ë¡œ í†µì‹ ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ë„¤íŠ¸ì›Œí¬ ë¦¬ì†ŒìŠ¤.                                                                               |
| Pod                             | ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ê°€ì¥ ê¸°ë³¸ì ì¸ ì‹¤í–‰ ë‹¨ìœ„                                                           | ì»¨í…Œì´ë„ˆ(Container)                  | `kubectl get pods`          | ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì‹¤ì œ ì‹¤í–‰ í™˜ê²½. ì»¨í…Œì´ë„ˆë¥¼ ë˜í•‘í•˜ê³ , ë„¤íŠ¸ì›Œí¬ì™€ ìŠ¤í† ë¦¬ì§€ë¥¼ ì—°ê²°í•¨.                                                                                         |
| Deployment                      | ì• í”Œë¦¬ì¼€ì´ì…˜ ë°°í¬ ë° ê´€ë¦¬ (ReplicaSet ìƒì„± ë° ê´€ë¦¬)                                                | Pod, ReplicaSet                     | `kubectl get deploy`        | ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ìƒíƒœë¥¼ ì„ ì–¸ì ìœ¼ë¡œ ê´€ë¦¬. ìŠ¤ì¼€ì¼ë§(í™•ì¥/ì¶•ì†Œ)ê³¼ ë¡¤ì•„ì›ƒ(ì—…ë°ì´íŠ¸), ë¡¤ë°±ì„ ì œê³µ.                                                                               |
| StatefulSet                     | ìƒíƒœë¥¼ ê°€ì§„ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ë°°í¬ ë° ê´€ë¦¬                                                           | PVC, Pod                            | `kubectl get statefulset`   | Stateful ì• í”Œë¦¬ì¼€ì´ì…˜ (ì˜ˆ: ë°ì´í„°ë² ì´ìŠ¤)ì„ ê´€ë¦¬. Podì˜ ìˆœì„œì™€ ìƒíƒœë¥¼ ìœ ì§€í•˜ë©° PVCì™€ ê¸´ë°€í•˜ê²Œ ì—°ê²°ë¨.                                                                      |
| ConfigMap                       | ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ìš©í•  êµ¬ì„± ë°ì´í„°ë¥¼ ì €ì¥                                                        | Pod, Deployment                     | `kubectl get configmap`     | í‚¤-ê°’ ìŒìœ¼ë¡œ êµ¬ì„± ë°ì´í„°ë¥¼ ì €ì¥. Podì—ì„œ í™˜ê²½ ë³€ìˆ˜ë‚˜ íŒŒì¼ í˜•íƒœë¡œ ì‚¬ìš© ê°€ëŠ¥.                                                                                                |
| Secret                          | ë¯¼ê°í•œ ë°ì´í„°ë¥¼ ì €ì¥ (ì˜ˆ: ë¹„ë°€ë²ˆí˜¸, ì¸ì¦ í† í°)                                                    | Pod, Deployment                     | `kubectl get secret`        | ì•”í˜¸í™”ëœ ë°ì´í„° ì €ì¥ì†Œ. ConfigMapê³¼ ìœ ì‚¬í•˜ë‚˜, ë¯¼ê°í•œ ë°ì´í„°ë¥¼ ë‹¤ë£° ë•Œ ì‚¬ìš©.                                                                                                |
| Ingress                         | ì™¸ë¶€ ìš”ì²­ì„ í´ëŸ¬ìŠ¤í„° ë‚´ì˜ Serviceë¡œ ë¼ìš°íŒ…                                                        | Service, Pod                        | `kubectl get ingress`       | HTTP(S) íŠ¸ë˜í”½ì„ ë¼ìš°íŒ…í•˜ì—¬ í´ëŸ¬ìŠ¤í„° ì™¸ë¶€ì—ì„œ ë‚´ë¶€ë¡œ ì—°ê²°. ë¡œë“œë°¸ëŸ°ì„œì™€ ê°™ì€ ì—­í•  ìˆ˜í–‰.                                                                                   |
| Namespace                       | í´ëŸ¬ìŠ¤í„° ë¦¬ì†ŒìŠ¤ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ê´€ë¦¬                                                         | ëª¨ë“  ë¦¬ì†ŒìŠ¤                          | `kubectl get ns`            | Kubernetes í´ëŸ¬ìŠ¤í„° ë‚´ì˜ ë¦¬ì†ŒìŠ¤ë¥¼ ê·¸ë£¹í™”í•˜ì—¬ ê²©ë¦¬í•˜ê³  ê´€ë¦¬í•˜ê¸° ìœ„í•œ ë…¼ë¦¬ì  ë‹¨ìœ„.                                                                                          |
| Node                            | Kubernetes í´ëŸ¬ìŠ¤í„°ì˜ ë¬¼ë¦¬ì  ë˜ëŠ” ê°€ìƒ ì„œë²„                                                       | Pod                                 | `kubectl get nodes`         | í´ëŸ¬ìŠ¤í„°ë¥¼ êµ¬ì„±í•˜ëŠ” ë¬¼ë¦¬ì /ê°€ìƒ ë¨¸ì‹ . PodëŠ” Nodeì—ì„œ ì‹¤í–‰ë˜ë©°, ìì› ì‚¬ìš©ëŸ‰ì„ ê´€ë¦¬.                                                                                         |
| DaemonSet                       | ê° Nodeì—ì„œ íŠ¹ì • Podë¥¼ ë°°í¬í•˜ê¸° ìœ„í•œ ë¦¬ì†ŒìŠ¤                                                       | Node, Pod                           | `kubectl get daemonset`     | í´ëŸ¬ìŠ¤í„°ì˜ ëª¨ë“  ë…¸ë“œì— ë™ì¼í•œ Podë¥¼ ë°°í¬. ì˜ˆ: ë¡œê·¸ ìˆ˜ì§‘, ëª¨ë‹ˆí„°ë§ ì—ì´ì „íŠ¸.                                                                                                |
| ReplicaSet                      | íŠ¹ì • ìˆ˜ì˜ Pod ë³µì œë¥¼ ìœ ì§€                                                                          | Pod, Deployment                     | `kubectl get rs`            | Podì˜ ìˆ˜ë¥¼ ë³´ì¥í•˜ê¸° ìœ„í•œ ì»¨íŠ¸ë¡¤ëŸ¬. Deploymentê°€ ReplicaSetì„ ê´€ë¦¬.                                                                                                         |
| Job                             | í•˜ë‚˜ì˜ ì‘ì—…ì„ ì™„ë£Œí•˜ê¸° ìœ„í•œ Pod ì‹¤í–‰                                                              | Pod                                 | `kubectl get jobs`          | ë‹¨ì¼ ì‘ì—…(ì˜ˆ: ë°ì´í„° ì²˜ë¦¬)ì„ ì‹¤í–‰í•˜ê³  ì¢…ë£Œ. ë°˜ë³µ ì‹¤í–‰ì´ í•„ìš” ì—†ë‹¤ë©´ Jobì„ ì‚¬ìš©.                                                                                            |
| CronJob                         | ì •ê¸°ì ìœ¼ë¡œ Job ì‹¤í–‰                                                                               | Job, Pod                            | `kubectl get cronjob`       | í¬ë¡  ìŠ¤ì¼€ì¤„ëŸ¬ì™€ ìœ ì‚¬. ì •í•´ì§„ ì‹œê°„ ê°„ê²©ìœ¼ë¡œ Jobì„ ì‹¤í–‰.                                                                                                                     |
| StorageClass                    | PVë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•˜ê¸° ìœ„í•œ í…œí”Œë¦¿                                                                | PV, PVC                             | `kubectl get sc`            | PVC ìš”ì²­ì— ë”°ë¼ Kubernetesê°€ ë™ì ìœ¼ë¡œ PVë¥¼ ìƒì„±í•  ë•Œ í•„ìš”í•œ ì •ì˜. í´ë¼ìš°ë“œ í™˜ê²½ì—ì„œ ì£¼ë¡œ ì‚¬ìš©.                                                                            |
| Endpoint                        | Serviceì™€ ì—°ê²°ëœ Podì˜ IP ë° í¬íŠ¸ë¥¼ ê´€ë¦¬                                                          | Service, Pod                        | `kubectl get endpoints`     | Serviceê°€ ë¼ìš°íŒ…í•˜ëŠ” ëŒ€ìƒ Podì˜ ë„¤íŠ¸ì›Œí¬ ì •ë³´ë¥¼ ê´€ë¦¬.                                                                                                                      |

## í™˜ê²½êµ¬ì„±
ìš°ì„  ì•„ë˜í‘œë¥¼ ì •ë¦¬í•˜ê³  Xen Hypervisorë¥¼ í†µí•´ì„œ VMì„ ìƒì„± í›„ ì§„í–‰.

| IP           | Hostname        | Describe                 |
|:------------:|:----------------|:-------------------------|
| 10.1.81.240  | haproxy-VIP     | HAProxy VIP              |
| 10.1.81.241  | master-node01   | Kubernetes Control Plane |
| 10.1.81.242  | master-node02   | Kubernetes Control Plane |
| 10.1.81.243  | master-node03   | Kubernetes Control Plane |
| 10.1.81.244  | worker-node01   | Kubernetes Woker node    |
| 10.1.81.245  | worker-node02   | Kubernetes Woker node    |
| 10.1.81.246  | haproxy01       | HAProxy + Keepalived LB  |
| 10.1.81.247  | haproxy02       | HAProxy + Keepalived LB  |
| 10.1.81.248  | postgres01      | PostgresSQL Database     |
| 10.1.81.249  | postgres02      | PostgresSQL Database     |

### SSH í‚¤ ìƒì„± ë° ë³µì‚¬
1. SSH í‚¤ ìƒì„± (ed25519 ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©)
2. SSH í‚¤ ê° ë…¸ë“œì— ë³µì‚¬ (Control Plane, Worker ë…¸ë“œ)

```bash
echo "yes" | ssh-keygen -t ed25519 -N "" -f /root/.ssh/id_ed25519 -C $(hostname -s)
```

```bash
ssh-copy-id -i /root/.ssh/id_ed25519.pub 10.1.81.241
ssh-copy-id -i /root/.ssh/id_ed25519.pub 10.1.81.242
ssh-copy-id -i /root/.ssh/id_ed25519.pub 10.1.81.243
ssh-copy-id -i /root/.ssh/id_ed25519.pub 10.1.81.244
ssh-copy-id -i /root/.ssh/id_ed25519.pub 10.1.81.245
```

### HA ì„¤ì •

#### Keepalived ì„¤ì¹˜
1. Keepalived ì„¤ì¹˜
2. `/etc/keepalived/keepalived.conf`{: filepath} MASTER/BACKUP Keepalived ì„¤ì • íŒŒì¼ í¸ì§‘
3. Keepalived ì„œë¹„ìŠ¤ ì¬ì‹œì‘

```bash
sudo apt update
sudo apt install keepalived haproxy
```

MASTER ì„œë²„ ì„¤ì •

<details markdown="block" style="margin: 1em 0; padding: 0.8em; border: 2px solid #007acc; border-radius: 10px; background-color: #f5faff; box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);">
  <summary>
    í¼ì¹˜ê¸°/ì ‘ê¸°
  </summary>

```bash
cat <<EOF > /etc/keepalived/keepalived.conf
! Configuration File for keepalived
  
global_defs {
    notification_email {
        idc@infra.com               # ì•Œë¦¼ ì´ë©”ì¼ì„ ë°›ì„ ì£¼ì†Œ
    }

    notification_email_from no-reply@infra.com

    smtp_server 127.0.0.1           # SMTP ì„œë²„ ì£¼ì†Œ
    smtp_connect_timeout 60         # SMTP ì„œë²„ ì—°ê²° íƒ€ì„ì•„ì›ƒ

    router_id LVS_MAIN              # ë¼ìš°í„° ID (ì„œë²„ ì‹ë³„ìš©)
    
    vrrp_skip_check_adv_addr        # Advertisement Address ê²€ì‚¬ ë¹„í™œì„±í™”
    vrrp_garp_interval 0.000001     # GARP íŒ¨í‚· ë¹„í™œì„±í™” (IPv4)
    vrrp_gna_interval 0.000001      # GNA íŒ¨í‚· ë¹„í™œì„±í™” (IPv6)
    
    script_user root
    enable_script_security          # ìŠ¤í¬ë¦½íŠ¸ ë³´ì•ˆ í™œì„±í™”
}

vrrp_script chk_haproxy {
    script "/usr/bin/killall -0 haproxy"
    interval 2
    weight 2
}

# Internal VRRP ì¸ìŠ¤í„´ìŠ¤
vrrp_instance K8s-VIP {
    state MASTER                    # MASTER/BACKUP ì¤‘ ì„ íƒ (ì£¼ ì„œë²„ì—ì„œëŠ” MASTER)
    interface enX0                  # VIPë¥¼ í• ë‹¹í•  ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤
    virtual_router_id 240           # VRRP ê·¸ë£¹ ID (ë²”ìœ„: 1 ~255, ê°™ì€ ê·¸ë£¹ì—ì„œëŠ” ë™ì¼í•´ì•¼ í•¨)
    priority 101                    # ìš°ì„ ìˆœìœ„ (ìˆ«ìê°€ ë†’ì„ìˆ˜ë¡ ìš°ì„ )
    smtp_alert                      # VRRP ì¸ìŠ¤í„´ìŠ¤ ìƒíƒœ ë³€í™” ì‹œ ì´ë©”ì¼ë¡œ ì•Œë¦¼ ì „ì†¡
    advert_int 1                    # VRRP ì‹ í˜¸ ê°„ê²© (ì´ˆ ë‹¨ìœ„)


    authentication {
        auth_type PASS
        auth_pass 1234              # VRRP ì¸ì¦ ë¹„ë°€ë²ˆí˜¸
    }

    unicast_src_ip 10.1.81.246      # í˜„ì¬ ë…¸ë“œì˜ ì†ŒìŠ¤ IP
    unicast_peer {
        10.1.81.247                 # ë‹¤ë¥¸ Keepalived ì¸ìŠ¤í„´ìŠ¤ì˜ IP ì£¼ì†Œ
    }

    virtual_ipaddress {
        10.1.81.240/32              # VIP ì„¤ì •
    }

    track_script {
        chk_haproxy                 #  ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ HAProxy ìƒíƒœë¥¼ í™•ì¸
    }
}
EOF
```

</details>

BACKUP ì„œë²„ ì„¤ì •

<details markdown="block" style="margin: 1em 0; padding: 0.8em; border: 2px solid #007acc; border-radius: 10px; background-color: #f5faff; box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);">
  <summary>
    í¼ì¹˜ê¸°/ì ‘ê¸°
  </summary>

```bash
cat <<EOF > /etc/keepalived/keepalived.conf
! Configuration File for keepalived
  
global_defs {
    notification_email {
        idc@infra.com               # ì•Œë¦¼ ì´ë©”ì¼ì„ ë°›ì„ ì£¼ì†Œ
    }

    notification_email_from no-reply@infra.com

    smtp_server 127.0.0.1           # SMTP ì„œë²„ ì£¼ì†Œ
    smtp_connect_timeout 60         # SMTP ì„œë²„ ì—°ê²° íƒ€ì„ì•„ì›ƒ

    router_id LVS_MAIN              # ë¼ìš°í„° ID (ì„œë²„ ì‹ë³„ìš©)
    
    vrrp_skip_check_adv_addr        # Advertisement Address ê²€ì‚¬ ë¹„í™œì„±í™”
    vrrp_garp_interval 0.000001     # GARP íŒ¨í‚· ë¹„í™œì„±í™” (IPv4)
    vrrp_gna_interval 0.000001      # GNA íŒ¨í‚· ë¹„í™œì„±í™” (IPv6)
    
    script_user root
    enable_script_security          # ìŠ¤í¬ë¦½íŠ¸ ë³´ì•ˆ í™œì„±í™”
}

vrrp_script chk_haproxy {
    script "/usr/bin/killall -0 haproxy"
    interval 2
    weight 2
}

# Internal VRRP ì¸ìŠ¤í„´ìŠ¤
vrrp_instance K8s-VIP {
    state BACKUP                    # MASTER/BACKUP ì¤‘ ì„ íƒ (ì£¼ ì„œë²„ì—ì„œëŠ” MASTER)
    interface enX0                  # VIPë¥¼ í• ë‹¹í•  ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤
    virtual_router_id 240           # VRRP ê·¸ë£¹ ID (ë²”ìœ„: 1 ~255, ê°™ì€ ê·¸ë£¹ì—ì„œëŠ” ë™ì¼í•´ì•¼ í•¨)
    priority 100                    # ìš°ì„ ìˆœìœ„ (ìˆ«ìê°€ ë†’ì„ìˆ˜ë¡ ìš°ì„ )
    smtp_alert                      # VRRP ì¸ìŠ¤í„´ìŠ¤ ìƒíƒœ ë³€í™” ì‹œ ì´ë©”ì¼ë¡œ ì•Œë¦¼ ì „ì†¡
    advert_int 1                    # VRRP ì‹ í˜¸ ê°„ê²© (ì´ˆ ë‹¨ìœ„)


    authentication {
        auth_type PASS
        auth_pass 1234              # VRRP ì¸ì¦ ë¹„ë°€ë²ˆí˜¸
    }

    unicast_src_ip 10.1.81.247      # í˜„ì¬ ë…¸ë“œì˜ ì†ŒìŠ¤ IP
    unicast_peer {
        10.1.81.246                 # ë‹¤ë¥¸ Keepalived ì¸ìŠ¤í„´ìŠ¤ì˜ IP ì£¼ì†Œ
    }

    virtual_ipaddress {
        10.1.81.240/32              # VIP ì„¤ì •
    }

    track_script {
        chk_haproxy                 #  ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì—¬ HAProxy ìƒíƒœë¥¼ í™•ì¸
    }
}
EOF
```

</details>

```bash
systemctl restart keepalived.service
```


#### Haproxy ì„¤ì¹˜
1. Haproxy ì„¤ì¹˜
2. `/etc/haproxy/haproxy.cfg`{: filepath} Haproxy ì„¤ì • íŒŒì¼ í¸ì§‘
3. Haproxy ì„œë¹„ìŠ¤ ì¬ì‹œì‘

<details markdown="block" style="margin: 1em 0; padding: 0.8em; border: 2px solid #007acc; border-radius: 10px; background-color: #f5faff; box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);">
  <summary>
    í¼ì¹˜ê¸°/ì ‘ê¸°
  </summary>

```bash
cat <<'EOF' > /etc/haproxy/haproxy.cfg
#--------------------------------------------------------------------#
# Haproxy settings                                                   #
#--------------------------------------------------------------------#

#--------------------------------------------------------------------#
# Global settings                                                    #
#--------------------------------------------------------------------#
global
	log /dev/log	local0
	log /dev/log	local1 notice
	chroot /var/lib/haproxy
	stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners
	stats timeout 30s
	user haproxy
	group haproxy
	daemon

	# Default SSL material locations
	ca-base /etc/ssl/certs
	crt-base /etc/ssl/private

	# See: https://ssl-config.mozilla.org/#server=haproxy&server-version=2.0.3&config=intermediate
        ssl-default-bind-ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384
        ssl-default-bind-ciphersuites TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256
        ssl-default-bind-options ssl-min-ver TLSv1.2 no-tls-tickets

#--------------------------------------------------------------------#
# Default Configuration                                              #
#--------------------------------------------------------------------#
defaults
        balance source
	log	global
#	mode	http
	option	httplog
	option	dontlognull
        timeout connect 5s
        timeout client  5m
        timeout server  30m
        timeout tunnel  24d

#--------------------------------------------------------------------#
# Frontend Configuration                                             #
#--------------------------------------------------------------------#
frontend k8s_api_front
        mode    tcp
        bind    *:8383
#        bind    *:6443 ssl crt /etc/haproxy/ssl/server.pem alpn h2,http/1.1
        option  tcplog

        # Access Control List
        tcp-request connection accept if { src 127.0.0.1 }
        tcp-request connection accept if { src 10.1.81.0/24 }
        tcp-request connection reject

        # Backend Access Control List
        use_backend k8s_api_back

#--------------------------------------------------------------------#
# BackEnd Platform Configuration                                     #
#--------------------------------------------------------------------#
backend k8s_api_back
        mode    tcp
        balance source
        option log-health-checks
        default-server check inter 5s fastinter 1s rise 2 fall 3

        server control-node01 10.1.81.241:6443
        server control-node02 10.1.81.242:6443
        server control-node03 10.1.81.243:6443

#--------------------------------------------------------------------#
# HAProxy Monitoring Configuration                                   #
#--------------------------------------------------------------------#
listen stats
	mode	http
        bind    *:32000
        stats enable
        stats realm Kubernetes Haproxy       # ë¸Œë¼ìš°ì € íƒ€ì´í‹€
        stats uri /                          # stat ë¥¼ ì œê³µí•  URI
        # ì ‘ê·¼ ì œí•œ ì„¤ì •
        http-request deny if !{ src 127.0.0.1 } !{ src 10.1.81.0/24 }
        # ì‚¬ìš©ì ì¸ì¦ ACL ì„¤ì •
        acl Auth http_auth(crdentials)
        http-request auth if !Auth

# ì‚¬ìš©ì ëª©ë¡
userlist crdentials
        user k8s_mon password $5$Ku7NZ.d3iw6zGckc$hHJ0RV.rjwhKDjhpdzAaJcZeOsFphxynEfguUK9OG99
EOF
```

</details>

## Kubespray ì„¤ì¹˜

### Kubespray í´ë¡  ë° ì¤€ë¹„

```bash
git ls-remote --heads --tags https://github.com/kubernetes-sigs/kubespray.git
git clone -b v2.25.0 https://github.com/kubernetes-sigs/kubespray.git
cd kubespray
```

### Python í™˜ê²½ ì„¤ì • ë° ì˜ì¡´ì„± ì„¤ì¹˜

```bash
sudo apt install python3-pip
sudo apt install -y python3-venv
python3 -m venv venv
source venv/bin/activate
python3 -m pip install -r requirements.txt
```

### Kubespray Inventory ë³µì‚¬

```bash
cp -rfpv inventory/sample inventory/awx
```

### Inventory íŒŒì¼ êµ¬ì„±
1. `all.yml`{: .filepath} HAProxyë¡œ ì„¤ì •ëœ Load Balancerì˜ IPì™€ í¬íŠ¸ ì§€ì •

<details markdown="block" style="margin: 1em 0; padding: 0.8em; border: 2px solid #007acc; border-radius: 10px; background-color: #f5faff; box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);">
  <summary>
    í¼ì¹˜ê¸°/ì ‘ê¸°
  </summary>

```yaml
---
## Directory where the binaries will be installed
bin_dir: /usr/local/bin

## The access_ip variable is used to define how other nodes should access
## the node.  This is used in flannel to allow other flannel nodes to see
## this node for example.  The access_ip is really useful AWS and Google
## environments where the nodes are accessed remotely by the "public" ip,
## but don't know about that address themselves.
# access_ip: 1.1.1.1


## External LB example config
## apiserver_loadbalancer_domain_name: "elb.some.domain"
loadbalancer_apiserver:
  address: 10.1.81.240
  port: 8383

## Internal loadbalancers for apiservers
# loadbalancer_apiserver_localhost: true
# valid options are "nginx" or "haproxy"
# loadbalancer_apiserver_type: nginx  # valid values "nginx" or "haproxy"

## Local loadbalancer should use this port
## And must be set port 6443
loadbalancer_apiserver_port: 6443

## If loadbalancer_apiserver_healthcheck_port variable defined, enables proxy liveness check for nginx.
loadbalancer_apiserver_healthcheck_port: 8081

### OTHER OPTIONAL VARIABLES

## By default, Kubespray collects nameservers on the host. It then adds the previously collected nameservers in nameserverentries.
## If true, Kubespray does not include host nameservers in nameserverentries in dns_late stage. However, It uses the nameserver to make sure cluster installed safely in dns_early stage.
## Use this option with caution, you may need to define your dns servers. Otherwise, the outbound queries such as www.google.com may fail.
# disable_host_nameservers: false

## Upstream dns servers
# upstream_dns_servers:
#   - 8.8.8.8
#   - 8.8.4.4

## There are some changes specific to the cloud providers
## for instance we need to encapsulate packets with some network plugins
## If set the possible values are either 'gce', 'aws', 'azure', 'openstack', 'vsphere', 'oci', or 'external'
## When openstack is used make sure to source in the openstack credentials
## like you would do when using openstack-client before starting the playbook.
# cloud_provider:

## When cloud_provider is set to 'external', you can set the cloud controller to deploy
## Supported cloud controllers are: 'openstack', 'vsphere', 'huaweicloud' and 'hcloud'
## When openstack or vsphere are used make sure to source in the required fields
# external_cloud_provider:

## Set these proxy values in order to update package manager and docker daemon to use proxies and custom CA for https_proxy if needed
# http_proxy: ""
# https_proxy: ""
# https_proxy_cert_file: ""

## Refer to roles/kubespray-defaults/defaults/main/main.yml before modifying no_proxy
# no_proxy: ""

## Some problems may occur when downloading files over https proxy due to ansible bug
## https://github.com/ansible/ansible/issues/32750. Set this variable to False to disable
## SSL validation of get_url module. Note that kubespray will still be performing checksum validation.
# download_validate_certs: False

## If you need exclude all cluster nodes from proxy and other resources, add other resources here.
# additional_no_proxy: ""

## If you need to disable proxying of os package repositories but are still behind an http_proxy set
## skip_http_proxy_on_os_packages to true
## This will cause kubespray not to set proxy environment in /etc/yum.conf for centos and in /etc/apt/apt.conf for debian/ubuntu
## Special information for debian/ubuntu - you have to set the no_proxy variable, then apt package will install from your source of wish
# skip_http_proxy_on_os_packages: false

## Since workers are included in the no_proxy variable by default, docker engine will be restarted on all nodes (all
## pods will restart) when adding or removing workers.  To override this behaviour by only including master nodes in the
## no_proxy variable, set below to true:
no_proxy_exclude_workers: false

## Certificate Management
## This setting determines whether certs are generated via scripts.
## Chose 'none' if you provide your own certificates.
## Option is  "script", "none"
# cert_management: script

## Set to true to allow pre-checks to fail and continue deployment
# ignore_assert_errors: false

## The read-only port for the Kubelet to serve on with no authentication/authorization. Uncomment to enable.
# kube_read_only_port: 10255

## Set true to download and cache container
# download_container: true

## Deploy container engine
# Set false if you want to deploy container engine manually.
# deploy_container_engine: true

## Red Hat Enterprise Linux subscription registration
## Add either RHEL subscription Username/Password or Organization ID/Activation Key combination
## Update RHEL subscription purpose usage, role and SLA if necessary
# rh_subscription_username: ""
# rh_subscription_password: ""
# rh_subscription_org_id: ""
# rh_subscription_activation_key: ""
# rh_subscription_usage: "Development"
# rh_subscription_role: "Red Hat Enterprise Server"
# rh_subscription_sla: "Self-Support"

## Check if access_ip responds to ping. Set false if your firewall blocks ICMP.
# ping_access_ip: true

# sysctl_file_path to add sysctl conf to
# sysctl_file_path: "/etc/sysctl.d/99-sysctl.conf"

## Variables for webhook token auth https://kubernetes.io/docs/reference/access-authn-authz/authentication/#webhook-token-authentication
kube_webhook_token_auth: false
kube_webhook_token_auth_url_skip_tls_verify: false
# kube_webhook_token_auth_url: https://...
## base64-encoded string of the webhook's CA certificate
# kube_webhook_token_auth_ca_data: "LS0t..."

## NTP Settings
# Start the ntpd or chrony service and enable it at system boot.
ntp_enabled: false
ntp_manage_config: false
ntp_servers:
  - "0.pool.ntp.org iburst"
  - "1.pool.ntp.org iburst"
  - "2.pool.ntp.org iburst"
  - "3.pool.ntp.org iburst"

## Used to control no_log attribute
unsafe_show_logs: false

## If enabled it will allow kubespray to attempt setup even if the distribution is not supported. For unsupported distributions this can lead to unexpected failures in some cases.
allow_unsupported_distribution_setup: false
```
{: file='inventory/awx/group_vars/all/all.yml'}

</details>

2. `addons.yml`{: .filepath} ì¶”ê°€ ì• ë“œì˜¨(ëŒ€ì‹œë³´ë“œ, Helm, Metrics Server ë“±)ì„ í™œì„±í™”

<details markdown="block" style="margin: 1em 0; padding: 0.8em; border: 2px solid #007acc; border-radius: 10px; background-color: #f5faff; box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);">
  <summary>
    í¼ì¹˜ê¸°/ì ‘ê¸°
  </summary>

```yaml
---
# Kubernetes dashboard
# RBAC required. see docs/getting-started.md for access details.
dashboard_enabled: true

# Helm deployment
helm_enabled: true

# Registry deployment
registry_enabled: true
registry_namespace: kube-system
registry_storage_class: ""
registry_disk_size: "10Gi"

# Metrics Server deployment
metrics_server_enabled: true
metrics_server_container_port: 10250
metrics_server_kubelet_insecure_tls: true
metrics_server_metric_resolution: 15s
metrics_server_kubelet_preferred_address_types: "InternalIP,ExternalIP,Hostname"
metrics_server_host_network: false
metrics_server_replicas: 1

# Rancher Local Path Provisioner
local_path_provisioner_enabled: true
local_path_provisioner_namespace: "local-path-storage"
local_path_provisioner_storage_class: "local-path"
local_path_provisioner_reclaim_policy: Delete
local_path_provisioner_claim_root: /data/volumes/opt/local-path-provisioner/
local_path_provisioner_debug: false
local_path_provisioner_image_repo: "{{ docker_image_repo }}/rancher/local-path-provisioner"
local_path_provisioner_image_tag: "v0.0.24"
local_path_provisioner_helper_image_repo: "busybox"
local_path_provisioner_helper_image_tag: "latest"

# Local volume provisioner deployment
local_volume_provisioner_enabled: true
local_volume_provisioner_namespace: kube-system
local_volume_provisioner_nodelabels:
  - kubernetes.io/hostname
  - topology.kubernetes.io/region
  - topology.kubernetes.io/zone
local_volume_provisioner_storage_classes:
  local-storage:
    host_dir: /data/volumes
    mount_dir: /data/volumes
    volume_mode: Filesystem
    fs_type: ext4
  fast-disks:
    host_dir: /data/volumes/fast-disks
    mount_dir: /data/volumes/fast-disks
    block_cleaner_command:
      - "/scripts/shred.sh"
      - "2"
    volume_mode: Filesystem
    fs_type: ext4
local_volume_provisioner_tolerations:
  - effect: NoSchedule
    operator: Exists

# CSI Volume Snapshot Controller deployment, set this to true if your CSI is able to manage snapshots
# currently, setting cinder_csi_enabled=true would automatically enable the snapshot controller
# Longhorn is an external CSI that would also require setting this to true but it is not included in kubespray
# csi_snapshot_controller_enabled: false
# csi snapshot namespace
# snapshot_controller_namespace: kube-system

# CephFS provisioner deployment
cephfs_provisioner_enabled: false
# cephfs_provisioner_namespace: "cephfs-provisioner"
# cephfs_provisioner_cluster: ceph
# cephfs_provisioner_monitors: "172.24.0.1:6789,172.24.0.2:6789,172.24.0.3:6789"
# cephfs_provisioner_admin_id: admin
# cephfs_provisioner_secret: secret
# cephfs_provisioner_storage_class: cephfs
# cephfs_provisioner_reclaim_policy: Delete
# cephfs_provisioner_claim_root: /volumes
# cephfs_provisioner_deterministic_names: true

# RBD provisioner deployment
rbd_provisioner_enabled: false
# rbd_provisioner_namespace: rbd-provisioner
# rbd_provisioner_replicas: 2
# rbd_provisioner_monitors: "172.24.0.1:6789,172.24.0.2:6789,172.24.0.3:6789"
# rbd_provisioner_pool: kube
# rbd_provisioner_admin_id: admin
# rbd_provisioner_secret_name: ceph-secret-admin
# rbd_provisioner_secret: ceph-key-admin
# rbd_provisioner_user_id: kube
# rbd_provisioner_user_secret_name: ceph-secret-user
# rbd_provisioner_user_secret: ceph-key-user
# rbd_provisioner_user_secret_namespace: rbd-provisioner
# rbd_provisioner_fs_type: ext4
# rbd_provisioner_image_format: "2"
# rbd_provisioner_image_features: layering
# rbd_provisioner_storage_class: rbd
# rbd_provisioner_reclaim_policy: Delete

# Gateway API CRDs
gateway_api_enabled: false
# gateway_api_experimental_channel: false

# Nginx ingress controller deployment
ingress_nginx_enabled: false
# ingress_nginx_host_network: false
# ingress_nginx_service_type: LoadBalancer
# ingress_nginx_service_nodeport_http: 30080
# ingress_nginx_service_nodeport_https: 30081
ingress_publish_status_address: ""
# ingress_nginx_nodeselector:
#   kubernetes.io/os: "linux"
# ingress_nginx_tolerations:
#   - key: "node-role.kubernetes.io/control-plane"
#     operator: "Equal"
#     value: ""
#     effect: "NoSchedule"
# ingress_nginx_namespace: "ingress-nginx"
# ingress_nginx_insecure_port: 80
# ingress_nginx_secure_port: 443
# ingress_nginx_configmap:
#   map-hash-bucket-size: "128"
#   ssl-protocols: "TLSv1.2 TLSv1.3"
# ingress_nginx_configmap_tcp_services:
#   9000: "default/example-go:8080"
# ingress_nginx_configmap_udp_services:
#   53: "kube-system/coredns:53"
# ingress_nginx_extra_args:
#   - --default-ssl-certificate=default/foo-tls
# ingress_nginx_termination_grace_period_seconds: 300
# ingress_nginx_class: nginx
# ingress_nginx_without_class: true
# ingress_nginx_default: false

# ALB ingress controller deployment
ingress_alb_enabled: false
# alb_ingress_aws_region: "us-east-1"
# alb_ingress_restrict_scheme: "false"
# Enables logging on all outbound requests sent to the AWS API.
# If logging is desired, set to true.
# alb_ingress_aws_debug: "false"

# Cert manager deployment
cert_manager_enabled: false
# cert_manager_namespace: "cert-manager"
# cert_manager_tolerations:
#   - key: node-role.kubernetes.io/control-plane
#     effect: NoSchedule
# cert_manager_affinity:
#  nodeAffinity:
#    preferredDuringSchedulingIgnoredDuringExecution:
#    - weight: 100
#      preference:
#        matchExpressions:
#        - key: node-role.kubernetes.io/control-plane
#          operator: In
#          values:
#          - ""
# cert_manager_nodeselector:
#   kubernetes.io/os: "linux"

# cert_manager_trusted_internal_ca: |
#   -----BEGIN CERTIFICATE-----
#   [REPLACE with your CA certificate]
#   -----END CERTIFICATE-----
# cert_manager_leader_election_namespace: kube-system

# cert_manager_dns_policy: "ClusterFirst"
# cert_manager_dns_config:
#   nameservers:
#     - "1.1.1.1"
#     - "8.8.8.8"

# cert_manager_controller_extra_args:
#   - "--dns01-recursive-nameservers-only=true"
#   - "--dns01-recursive-nameservers=1.1.1.1:53,8.8.8.8:53"

# MetalLB deployment
metallb_enabled: false
metallb_speaker_enabled: "{{ metallb_enabled }}"
metallb_namespace: "metallb-system"
# metallb_version: v0.13.9
# metallb_protocol: "layer2"
# metallb_port: "7472"
# metallb_memberlist_port: "7946"
# metallb_config:
#   speaker:
#     nodeselector:
#       kubernetes.io/os: "linux"
#     tolerations:
#       - key: "node-role.kubernetes.io/control-plane"
#         operator: "Equal"
#         value: ""
#         effect: "NoSchedule"
#   controller:
#     nodeselector:
#       kubernetes.io/os: "linux"
#     tolerations:
#       - key: "node-role.kubernetes.io/control-plane"
#         operator: "Equal"
#         value: ""
#         effect: "NoSchedule"
#   address_pools:
#     primary:
#       ip_range:
#         - 10.5.0.0/16
#       auto_assign: true
#     pool1:
#       ip_range:
#         - 10.6.0.0/16
#       auto_assign: true
#     pool2:
#       ip_range:
#         - 10.10.0.0/16
#       auto_assign: true
#   layer2:
#     - primary
#   layer3:
#     defaults:
#       peer_port: 179
#       hold_time: 120s
#     communities:
#       vpn-only: "1234:1"
#       NO_ADVERTISE: "65535:65282"
#     metallb_peers:
#         peer1:
#           peer_address: 10.6.0.1
#           peer_asn: 64512
#           my_asn: 4200000000
#           communities:
#             - vpn-only
#           address_pool:
#             - pool1
#         peer2:
#           peer_address: 10.10.0.1
#           peer_asn: 64513
#           my_asn: 4200000000
#           communities:
#             - NO_ADVERTISE
#           address_pool:
#             - pool2

argocd_enabled: false
# argocd_version: v2.11.0
# argocd_namespace: argocd
# Default password:
#   - https://argo-cd.readthedocs.io/en/stable/getting_started/#4-login-using-the-cli
#   ---
#   The initial password is autogenerated and stored in `argocd-initial-admin-secret` in the argocd namespace defined above.
#   Using the argocd CLI the generated password can be automatically be fetched from the current kubectl context with the command:
#   argocd admin initial-password -n argocd
#   ---
# Use the following var to set admin password
# argocd_admin_password: "password"

# The plugin manager for kubectl
krew_enabled: false
krew_root_dir: "/usr/local/krew"

# Kube VIP
kube_vip_enabled: false
# kube_vip_arp_enabled: true
# kube_vip_controlplane_enabled: true
# kube_vip_address: 192.168.56.120
# loadbalancer_apiserver:
#   address: "{{ kube_vip_address }}"
#   port: 6443
# kube_vip_interface: eth0
# kube_vip_services_enabled: false
# kube_vip_dns_mode: first
# kube_vip_cp_detect: false
# kube_vip_leasename: plndr-cp-lock
# kube_vip_enable_node_labeling: false

# Node Feature Discovery
node_feature_discovery_enabled: false
# node_feature_discovery_gc_sa_name: node-feature-discovery
# node_feature_discovery_gc_sa_create: false
# node_feature_discovery_worker_sa_name: node-feature-discovery
# node_feature_discovery_worker_sa_create: false
# node_feature_discovery_master_config:
#   extraLabelNs: ["nvidia.com"]
```
{: file='inventory/awx/group_vars/k8s_cluster/addons.yml'}

</details>

```bash
kubectl describe nodes | grep -i taint
Taints:             node-role.kubernetes.io/control-plane:NoSchedule
Taints:             node-role.kubernetes.io/control-plane:NoSchedule
Taints:             node-role.kubernetes.io/control-plane:NoSchedule
Taints:             <none>
Taints:             <none>
```
> `[kube_node]`ì— ì›Œì»¤ ë…¸ë“œë§Œ ì¶”ê°€ í•˜ì—¬ Podê°€ ìƒì„±ë˜ì§€ ì•Šë„ë¡ í•œë‹¤.
>  - ë§ˆìŠ¤í„° ë…¸ë“œ(Control Plane)ì— ì¼ë°˜ ì›Œí¬ë¡œë“œ Podê°€ ìƒì„±ë˜ì§€ ì•Šë„ë¡ Control plane ë…¸ë“œ ê²©ë¦¬
{: .prompt-info }

### Kubernetes ë‹¨ì¼ í´ëŸ¬ìŠ¤í„° ë°°í¬
1. `inventory.ini`{: .filepath} Inventory êµ¬ì„±

<details markdown="block" style="margin: 1em 0; padding: 0.8em; border: 2px solid #007acc; border-radius: 10px; background-color: #f5faff; box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);">
  <summary>
    í¼ì¹˜ê¸°/ì ‘ê¸°
  </summary>

```ini
# ## Configure 'ip' variable to bind kubernetes services on a
# ## different ip than the default iface
# ## We should set etcd_member_name for etcd cluster. The node that is not a etcd member do not need to set the value, or can set the empty string value.
[all]
control-node01 ansible_host=10.1.81.241 ip=10.1.81.241 etcd_member_name=etcd1
#control-node02 ansible_host=10.1.81.242 ip=10.1.81.242 etcd_member_name=etcd2
#control-node03 ansible_host=10.1.81.243 ip=10.1.81.243 etcd_member_name=etcd3
worker-node01  ansible_host=10.1.81.244 ip=10.1.81.244
worker-node02  ansible_host=10.1.81.245 ip=10.1.81.245

# ## configure a bastion host if your nodes are not directly reachable
# [bastion]
# bastion ansible_host=x.x.x.x ansible_user=some_user

[kube_control_plane]
control-node01
#control-node02
#control-node03

[etcd]
control-node01
#control-node02
#control-node03

[kube_node]
worker-node01
worker-node02

[calico_rr]

[k8s_cluster:children]
kube_control_plane
kube_node
calico_rr
```
{: file='inventory/awx/inventory.ini'}

</details>

2. í´ëŸ¬ìŠ¤í„° ì„¤ì •ì´ ì •ìƒì ì¸ì§€ í™•ì¸
3. Kubernetes í´ëŸ¬ìŠ¤í„°ë¥¼ ìƒì„±

```bash
ansible all -m ping -i inventory/awx/inventory.ini
ansible-playbook -i inventory/awx/inventory.ini --become --become-user=root cluster.yml
```

### Kubernetes HA í´ëŸ¬ìŠ¤í„° ë°°í¬
1. `inventory.ini`{: .filepath} Inventory êµ¬ì„±

<details markdown="block" style="margin: 1em 0; padding: 0.8em; border: 2px solid #007acc; border-radius: 10px; background-color: #f5faff; box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);">
  <summary>
    í¼ì¹˜ê¸°/ì ‘ê¸°
  </summary>

```ini
# ## Configure 'ip' variable to bind kubernetes services on a
# ## different ip than the default iface
# ## We should set etcd_member_name for etcd cluster. The node that is not a etcd member do not need to set the value, or can set the empty string value.
[all]
control-node01 ansible_host=10.1.81.241 ip=10.1.81.241 etcd_member_name=etcd1
control-node02 ansible_host=10.1.81.242 ip=10.1.81.242 etcd_member_name=etcd2
control-node03 ansible_host=10.1.81.243 ip=10.1.81.243 etcd_member_name=etcd3
worker-node01  ansible_host=10.1.81.244 ip=10.1.81.244
worker-node02  ansible_host=10.1.81.245 ip=10.1.81.245

# ## configure a bastion host if your nodes are not directly reachable
# [bastion]
# bastion ansible_host=x.x.x.x ansible_user=some_user

[kube_control_plane]
control-node01
control-node02
control-node03

[etcd]
control-node01
control-node02
control-node03

[kube_node]
worker-node01
worker-node02

[calico_rr]

[k8s_cluster:children]
kube_control_plane
kube_node
calico_rr
```
{: file='inventory/awx/inventory.ini'}

</details>

2. í´ëŸ¬ìŠ¤í„° ì„¤ì •ì´ ì •ìƒì ì¸ì§€ í™•ì¸
3. Kubernetes í´ëŸ¬ìŠ¤í„°ë¥¼ ìƒì„±

```bash
ansible all -m ping -i inventory/awx/inventory.ini
ansible-playbook -i inventory/awx/inventory.ini --become --become-user=root cluster.yml
```

### etcd ë° apiserver êµ¬ì„±
1. ì¸ì¦ì„œ ì¬ìƒì„±: VIPì™€ SANì„ í¬í•¨í•˜ì—¬ ì¸ì¦ì„œë¥¼ ì¬ìƒì„±

```bash
cd /etc/kubernetes/ssl/
mv -v apiserver.crt apiserver.crt.old
mv -v apiserver.key apiserver.key.old
```

```bash
kubeadm init phase certs apiserver \
  --apiserver-advertise-address=10.1.81.240 \
  --apiserver-cert-extra-sans="10.96.0.1,10.233.0.1,10.1.81.240,lb-apiserver.kubernetes.local"
```

```bash
openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout | grep "Subject Alternative Name" -A 1
            X509v3 Subject Alternative Name: 
                DNS:control-node01, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, DNS:lb-apiserver.kubernetes.local, IP Address:10.96.0.1, IP Address:10.1.81.240, IP Address:10.233.0.1
```

2. API Server êµ¬ì„± íŒŒì¼ ìˆ˜ì •: API Serverê°€ ëª¨ë“  etcd ë…¸ë“œì™€ í†µì‹ í•˜ë„ë¡ êµ¬ì„±

<details markdown="block" style="margin: 1em 0; padding: 0.8em; border: 2px solid #007acc; border-radius: 10px; background-color: #f5faff; box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);">
  <summary>
    í¼ì¹˜ê¸°/ì ‘ê¸°
  </summary>

```yaml
    - --etcd-servers=https://10.1.81.241:2379,https://10.1.81.242:2379,https://10.1.81.243:2379
```
{: file='/etc/kubernetes/manifests/kube-apiserver.yaml'}

</details>

3. ì¸ì¦ì„œ ë™ê¸°í™”: ë‹¤ë¥¸ Control Plane ë…¸ë“œì— ì¸ì¦ì„œ ë³µì‚¬

```bash
rsync -avhP 10.1.81.241:/etc/kubernetes/ssl/apiserver.{crt,key} /etc/kubernetes/ssl/
```

4. kubelet ì„œë¹„ìŠ¤ ì¬ì‹œì‘: ë³€ê²½ ì‚¬í•­ì„ ë°˜ì˜í•˜ê¸° ìœ„í•´ kubeletì„ ì¬ì‹œì‘

```bash
systemctl restart kubelet.service
```

### etcd ìƒíƒœ í™•ì¸

```bash
ETCDCTL_API=3 etcdctl --endpoints=https://10.1.81.241:2379,https://10.1.81.242:2379,https://10.1.81.243:2379 \
  --cacert=/etc/ssl/etcd/ssl/ca.pem \
  --cert=/etc/ssl/etcd/ssl/member-control-node01.pem \
  --key=/etc/ssl/etcd/ssl/member-control-node01-key.pem \
  endpoint health -w=table

ETCDCTL_API=3 etcdctl --endpoints=https://10.1.81.241:2379,https://10.1.81.242:2379,https://10.1.81.243:2379 \
  --cacert=/etc/ssl/etcd/ssl/ca.pem \
  --cert=/etc/ssl/etcd/ssl/member-control-node01.pem \
  --key=/etc/ssl/etcd/ssl/member-control-node01-key.pem \
  member list -w=table

ETCDCTL_API=3 etcdctl --endpoints=https://10.1.81.241:2379,https://10.1.81.242:2379,https://10.1.81.243:2379 \
  --cacert=/etc/ssl/etcd/ssl/ca.pem \
  --cert=/etc/ssl/etcd/ssl/member-control-node01.pem \
  --key=/etc/ssl/etcd/ssl/member-control-node01-key.pem \
  endpoint status --cluster -w=table
```

### í´ëŸ¬ìŠ¤í„° ë…¸ë“œ ìƒíƒœ í™•ì¸

```bash
kubectl get nodes -o wide
NAME             STATUS   ROLES           AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE           KERNEL-VERSION       CONTAINER-RUNTIME
control-node01   Ready    control-plane   6h54m   v1.30.4   10.1.81.241   <none>        Ubuntu 22.04 LTS   5.15.0-130-generic   containerd://1.7.21
control-node02   Ready    control-plane   4h5m    v1.30.4   10.1.81.242   <none>        Ubuntu 22.04 LTS   5.15.0-130-generic   containerd://1.7.21
control-node03   Ready    control-plane   4h4m    v1.30.4   10.1.81.243   <none>        Ubuntu 22.04 LTS   5.15.0-130-generic   containerd://1.7.21
worker-node01    Ready    <none>          6h53m   v1.30.4   10.1.81.244   <none>        Ubuntu 22.04 LTS   5.15.0-130-generic   containerd://1.7.21
worker-node02    Ready    <none>          6h53m   v1.30.4   10.1.81.245   <none>        Ubuntu 22.04 LTS   5.15.0-130-generic   containerd://1.7.21
```

### Kubectl ìë™ì™„ì„± ì„¤ì •

```bash
source <(kubectl completion bash)
echo "source <(kubectl completion bash)" >> ~/.bashrc
source <(kubeadm completion bash)
echo "source <(kubeadm completion bash)" >> ~/.bashrc
```

## ì°¸ì¡°
- [Kubespray ê³µì‹ ë¦¬í¬ì§€í† ë¦¬](https://github.com/kubernetes-sigs/kubespray)
- [Kubespray ê³µì‹ ë¬¸ì„œ](https://kubespray.io/)
